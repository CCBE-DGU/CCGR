{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBs5gC0fZNW9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Users\\dabin\\AppData\\Local\\Microsoft\\WindowsApps\\python3.11.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/dabin/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_curP8VYfyn"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Users\\dabin\\AppData\\Local\\Microsoft\\WindowsApps\\python3.11.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/dabin/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_1NwoXZ6oOp"
      },
      "outputs": [],
      "source": [
        "edge_data = pd.read_csv('Total_Edge/totaledge_edge_breast-cancer.csv')\n",
        "\n",
        "X = pd.read_csv('Total_Node/totalnode_brca_tcga.csv')\n",
        "X = pd.concat([X, pd.read_csv('Total_Node/totalnode_brca_tcga_pan_can_atlas_2018.csv')])\n",
        "X = pd.concat([X, pd.read_csv('Total_Node/totalnode_brca_tcga_pub2015.csv')])\n",
        "\n",
        "X = X.dropna()\n",
        "X = X.iloc[:, :-3]\n",
        "column_names = X.columns\n",
        "\n",
        "column_names_index, _ = pd.factorize(column_names)\n",
        "\n",
        "column_names_dic = {}\n",
        "for i, column_name in enumerate(column_names):\n",
        "    column_names_dic[column_name] = column_names_index[i]\n",
        "\n",
        "edge_data['source'] = edge_data['source'].map(column_names_dic)\n",
        "edge_data['target'] = edge_data['target'].map(column_names_dic)\n",
        "\n",
        "edge_data = edge_data[edge_data['weight'] != 0]\n",
        "sources = edge_data['source']\n",
        "targets = edge_data['target']\n",
        "\n",
        "edge_index = torch.tensor([sources, targets], dtype=torch.long)\n",
        "print(edge_index.shape)\n",
        "\n",
        "#edge_index생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzvqLozoACKc"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('Total_Node/totalnode_brca_tcga.csv')\n",
        "X = pd.concat([X, pd.read_csv('Total_Node/totalnode_brca_tcga_pan_can_atlas_2018.csv')])\n",
        "X = pd.concat([X, pd.read_csv('Total_Node/totalnode_brca_tcga_pub2015.csv')])\n",
        "\n",
        "path_list = ['breast-cancer']\n",
        "\n",
        "X = X.dropna()\n",
        "\n",
        "filtered_X = X.loc[X['path'] == path_list[0]]\n",
        "filtered_X = filtered_X.iloc[:, :-3]\n",
        "last_three_cols = X.iloc[:, -3:]\n",
        "y = last_three_cols.copy()\n",
        "y_2 = y.iloc[:, 1] #cancer_type_detailed\n",
        "\n",
        "X = filtered_X.values\n",
        "print(X)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X)\n",
        "X = sc.transform(X)\n",
        "\n",
        "y_2 = y_2.values\n",
        "\n",
        "unique_classes = np.unique(y_2)\n",
        "class_mapping = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
        "y_2_indices = [class_mapping[value] for value in y_2]\n",
        "y_2 = torch.tensor(y_2_indices)\n",
        "\n",
        "X = torch.tensor(X)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm7ny_vAhXKM"
      },
      "outputs": [],
      "source": [
        "num_samples = X.shape[0]\n",
        "\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.1\n",
        "indices = np.random.permutation(num_samples)\n",
        "\n",
        "# train, validation, test 인덱스 계산\n",
        "num_train = int(train_ratio * num_samples)\n",
        "num_val = int(val_ratio * num_samples)\n",
        "num_test = num_samples - num_train - num_val\n",
        "\n",
        "# 인덱스를 train, validation, test로 나누기\n",
        "train_indices = indices[:num_train]\n",
        "val_indices = indices[num_train:num_train+num_val]\n",
        "test_indices = indices[num_train+num_val:]\n",
        "\n",
        "train_mask = np.zeros(num_samples, dtype=int)\n",
        "train_mask[train_indices] = 1\n",
        "\n",
        "val_mask = np.zeros(num_samples, dtype=int)\n",
        "val_mask[val_indices] = 1\n",
        "\n",
        "test_mask = np.zeros(num_samples, dtype=int)\n",
        "test_mask[test_indices] = 1\n",
        "\n",
        "train_mask = torch.tensor(train_mask)\n",
        "val_mask = torch.tensor(val_mask)\n",
        "test_mask = torch.tensor(test_mask)\n",
        "\n",
        "data = Data(x=X, edge_index=edge_index, y=y_2,\n",
        "            train_mask=train_mask,  val_mask=val_mask, test_mask=test_mask)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWzGOlMgz2np"
      },
      "outputs": [],
      "source": [
        "print(data.edge_index)\n",
        "print(data.y)\n",
        "print(data.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdorsxIQZfCb"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim).double()\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim).double()\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim).double()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index= data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcamuGzxZiZ6"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = len(torch.unique(data.y))\n",
        "\n",
        "model = GCN(data.num_node_features, 32, num_classes).to(device)\n",
        "dataset = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-6)\n",
        "\n",
        "train_losses=[]\n",
        "train_accuracies=[]\n",
        "\n",
        "val_losses=[]\n",
        "val_accuracies=[]\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(dataset)\n",
        "    loss = F.nll_loss(out[dataset.train_mask==1], dataset.y[dataset.train_mask==1])\n",
        "    val_loss = F.nll_loss(out[dataset.val_mask==1], dataset.y[dataset.val_mask==1])\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Train accuracy\n",
        "    pred = out.argmax(dim=1)\n",
        "    train_correct = pred[dataset.train_mask==1] == dataset.y[dataset.train_mask==1]\n",
        "    train_acc = train_correct.sum().item() / dataset.train_mask.sum().item()\n",
        "\n",
        "    train_losses+=[loss.item()]\n",
        "    train_accuracies+=[train_acc]\n",
        "\n",
        "    # Validation accuracy\n",
        "    val_correct = pred[dataset.val_mask==1] == dataset.y[dataset.val_mask==1]\n",
        "    val_acc = val_correct.sum().item() / dataset.val_mask.sum().item()\n",
        "\n",
        "    val_losses+=[val_loss.item()]\n",
        "    val_accuracies+=[val_acc]\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghiwpLa4Ofba"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCTXFHoi-ck1"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "pred = model(dataset).argmax(dim=1)\n",
        "correct = (pred[dataset.train_mask == 1] == dataset.y[dataset.train_mask == 1]).sum()\n",
        "acc = int(correct) / int(dataset.train_mask.sum())\n",
        "print(f'Train Accuracy: {acc:f}')\n",
        "\n",
        "pred = model(dataset).argmax(dim=1)\n",
        "correct = (pred[dataset.test_mask == 1] == dataset.y[dataset.test_mask == 1]).sum()\n",
        "acc = int(correct) / int(dataset.test_mask.sum())\n",
        "print(f'Test Accuracy: {acc:f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
